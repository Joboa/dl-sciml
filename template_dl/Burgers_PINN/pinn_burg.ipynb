{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f3aa0825b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt   \n",
    "from scipy.io import loadmat\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "data = loadmat(\"data/burgers.mat\")\n",
    "x = data['x']\n",
    "t = data['t']\n",
    "usol = data['usol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, N_INPUT: int, N_OUTPUT: int, N_HIDDEN: int, N_LAYERS: int):\n",
    "        super(FCN).__init__()\n",
    "        self.activation = nn.Tanh\n",
    "        \n",
    "        # Input layer\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(N_INPUT, N_HIDDEN),  \n",
    "            self.activation()            \n",
    "        )\n",
    "        # Hidden layers\n",
    "        self.fch = self._create_hidden_layers(N_HIDDEN, N_LAYERS)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fce = nn.Linear(N_HIDDEN, N_OUTPUT)  # Output layer\n",
    "\n",
    "        # Xavier initialization\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _create_hidden_layers(self, N_HIDDEN: int, N_LAYERS: int) -> nn.Sequential:\n",
    "        # Creates hidden layers for the network.\n",
    "        layers = []\n",
    "        for _ in range(N_LAYERS - 1):\n",
    "            layers.append(nn.Linear(N_HIDDEN, N_HIDDEN)) \n",
    "            layers.append(self.activation())\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        input_tensor = torch.cat([x, t], dim=1)\n",
    "        x = self.fcs(input_tensor)\n",
    "        x = self.fch(x)\n",
    "        x = self.fce(x)\n",
    "        return x\n",
    "    \n",
    "    def net_u(self, x: torch.Tensor, t: torch.Tensor,):\n",
    "        return self.forward(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCN(2,1,20,8) # inputs, output, hidden_neurons, number_of_layers\n",
    "\n",
    "# Residual Computation [Physics Informed]\n",
    "def net_f(x, t, nu):\n",
    "    x.requires_grad_(True)\n",
    "    t.requires_grad_(True)\n",
    "\n",
    "    u = model.net_u(x, t)\n",
    "\n",
    "    if u is None:\n",
    "        raise ValueError(\"u is None. Check the net_u function.\")\n",
    "\n",
    "    u_t = torch.autograd.grad(u, t, \n",
    "                              grad_outputs=torch.ones_like(u), \n",
    "                              retain_graph=True,\n",
    "                              create_graph=True)[0]\n",
    "    \n",
    "    u_x = torch.autograd.grad(u, x, \n",
    "                              grad_outputs=torch.ones_like(u), \n",
    "                              retain_graph=True,\n",
    "                              create_graph=True)[0]\n",
    "    \n",
    "    if u_t is None or u_x is None:\n",
    "        raise ValueError(\"u_t or u_x is None. Check the gradient computation.\")\n",
    "    \n",
    "    u_xx = torch.autograd.grad(u_x, x, \n",
    "                               grad_outputs=torch.ones_like(u_x), \n",
    "                               retain_graph=True,\n",
    "                               create_graph=True)[0]\n",
    "    \n",
    "\n",
    "    if u_xx is None:\n",
    "        raise ValueError(\"u_xx is None. Check the second-order gradient computation.\")\n",
    "\n",
    "    f = u_t + u*u_x - nu * u_xx\n",
    "    return f\n",
    "\n",
    "# Boundary computation for boundary loss\n",
    "def net_b():\n",
    "    # Left boundary condition: initial condition at t=0\n",
    "    x_left = torch.linspace(-1, 1, 100).reshape(-1, 1)\n",
    "    t_left = torch.zeros_like(x_left)\n",
    "    u_left = -torch.sin(torch.pi * x_left)\n",
    "    \n",
    "    # Spatial boundary conditions at x=-1 and x=1 at different time points\n",
    "    t_bound = torch.linspace(0, 1, 100).reshape(-1, 1)\n",
    "    x_left_bound = torch.ones_like(t_bound) * (-1)\n",
    "    x_right_bound = torch.ones_like(t_bound) * (1)\n",
    "    u_left_bound = torch.zeros_like(t_bound)\n",
    "    u_right_bound = torch.zeros_like(t_bound)\n",
    "    \n",
    "    # Combine all boundary points\n",
    "    x_b = torch.cat([x_left, x_left_bound, x_right_bound], dim=0)\n",
    "    t_b = torch.cat([t_left, t_bound, t_bound], dim=0)\n",
    "    u_b = torch.cat([u_left, u_left_bound, u_right_bound], dim=0)\n",
    "    \n",
    "    return x_b, t_b, u_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ALL CODE ######\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "# Detect GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "data = loadmat(\"burgers_shock.mat\")\n",
    "x = torch.tensor(data['x'], dtype=torch.float32, device=device)\n",
    "t = torch.tensor(data['t'], dtype=torch.float32, device=device)\n",
    "usol = torch.tensor(data['usol'], dtype=torch.float32, device=device)\n",
    "\n",
    "# Kinematic viscosity\n",
    "nu = 0.01/np.pi\n",
    "\n",
    "# Plotting function for points\n",
    "def plot_points(x_b, t_b, x_f, t_f):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(x_b.cpu().numpy(), t_b.cpu().numpy(), color='red', label='Boundary Points')\n",
    "    plt.scatter(x_f.cpu().numpy(), t_f.cpu().numpy(), color='blue', label='Collocation Points')\n",
    "    plt.title('Sampling Points')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('t')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sampling_points.png')\n",
    "    plt.close()\n",
    "\n",
    "# Collocation points generation\n",
    "def sample_collocation_points(num_points=10000):\n",
    "    # Sample uniformly from the domain\n",
    "    x_f = torch.rand(num_points, 1, device=device) * 2 - 1  # x in [-1, 1]\n",
    "    t_f = torch.rand(num_points, 1, device=device)  # t in [0, 1]\n",
    "    return x_f, t_f\n",
    "\n",
    "# Boundary and initial condition points\n",
    "def net_b():\n",
    "    # Left boundary condition: initial condition at t=0\n",
    "    x_left = torch.linspace(-1, 1, 100).reshape(-1, 1).to(device)\n",
    "    t_left = torch.zeros_like(x_left)\n",
    "    u_left = -torch.sin(torch.pi * x_left)\n",
    "    \n",
    "    # Spatial boundary conditions at x=-1 and x=1 at different time points\n",
    "    t_bound = torch.linspace(0, 1, 100).reshape(-1, 1).to(device)\n",
    "    x_left_bound = torch.ones_like(t_bound) * (-1)\n",
    "    x_right_bound = torch.ones_like(t_bound) * (1)\n",
    "    u_left_bound = torch.zeros_like(t_bound)\n",
    "    u_right_bound = torch.zeros_like(t_bound)\n",
    "    \n",
    "    # Combine all boundary points\n",
    "    x_b = torch.cat([x_left, x_left_bound, x_right_bound], dim=0)\n",
    "    t_b = torch.cat([t_left, t_bound, t_bound], dim=0)\n",
    "    u_b = torch.cat([u_left, u_left_bound, u_right_bound], dim=0)\n",
    "    \n",
    "    return x_b, t_b, u_b\n",
    "\n",
    "# FCN class (with small modifications for device)\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, N_INPUT: int, N_OUTPUT: int, N_HIDDEN: int, N_LAYERS: int, device=device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.activation = nn.Tanh\n",
    "        # self.mu = mu\n",
    "        \n",
    "        # Input layer\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(N_INPUT, N_HIDDEN),  \n",
    "            self.activation()            \n",
    "        ).to(device)\n",
    "        # Hidden layers\n",
    "        self.fch = self._create_hidden_layers(N_HIDDEN, N_LAYERS).to(device)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fce = nn.Linear(N_HIDDEN, N_OUTPUT).to(device)\n",
    "\n",
    "        # Xavier initialization\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _create_hidden_layers(self, N_HIDDEN: int, N_LAYERS: int) -> nn.Sequential:\n",
    "        # Creates hidden layers for the network.\n",
    "        layers = []\n",
    "        for _ in range(N_LAYERS - 1):\n",
    "            layers.append(nn.Linear(N_HIDDEN, N_HIDDEN)) \n",
    "            layers.append(self.activation())\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        input_tensor = torch.cat([x, t], dim=1)\n",
    "        x = self.fcs(input_tensor)\n",
    "        x = self.fch(x)\n",
    "        x = self.fce(x)\n",
    "        return x\n",
    "    \n",
    "    def net_u(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        return self.forward(x,t)\n",
    "\n",
    "# Residual Computation [Physics Informed]\n",
    "def net_f(x, t, nu, model):\n",
    "    x.requires_grad_(True)\n",
    "    t.requires_grad_(True)\n",
    "\n",
    "    u = model.net_u(x, t)\n",
    "\n",
    "    u_t = torch.autograd.grad(u, t, \n",
    "                              grad_outputs=torch.ones_like(u), \n",
    "                              retain_graph=True,\n",
    "                              create_graph=True)[0]\n",
    "    \n",
    "    u_x = torch.autograd.grad(u, x, \n",
    "                              grad_outputs=torch.ones_like(u), \n",
    "                              retain_graph=True,\n",
    "                              create_graph=True)[0]\n",
    "    \n",
    "    u_xx = torch.autograd.grad(u_x, x, \n",
    "                               grad_outputs=torch.ones_like(u_x), \n",
    "                               retain_graph=True,\n",
    "                               create_graph=True)[0]\n",
    "\n",
    "    f = u_t + u*u_x - nu * u_xx\n",
    "    return f\n",
    "\n",
    "# Training loop\n",
    "def train_pinn(model, epochs=20000, learning_rate=1e-3):\n",
    "    # Prepare data\n",
    "    x_b, t_b, u_b = net_b()\n",
    "    x_f, t_f = sample_collocation_points()\n",
    "    \n",
    "    # Plot sampling points\n",
    "    plot_points(x_b, t_b, x_f, t_f)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Lists to store loss values\n",
    "    loss_history = []\n",
    "    boundary_loss_history = []\n",
    "    residual_loss_history = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Boundary loss\n",
    "        u_pred_b = model.net_u(x_b, t_b)\n",
    "        loss_b = torch.mean((u_pred_b - u_b)**2)\n",
    "        \n",
    "        # Residual loss (Physics Informed)\n",
    "        f_pred = net_f(x_f, t_f, nu, model)\n",
    "        loss_f = torch.mean(f_pred**2)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = loss_b + loss_f\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Store loss\n",
    "        loss_history.append(loss.item())\n",
    "        boundary_loss_history.append(loss_b.item())\n",
    "        residual_loss_history.append(loss_f.item())\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item():.4e}, Boundary Loss: {loss_b.item():.4e}, Residual Loss: {loss_f.item():.4e}')\n",
    "    \n",
    "    return loss_history, boundary_loss_history, residual_loss_history\n",
    "\n",
    "# Plot loss vs epochs\n",
    "def plot_loss(loss_history, boundary_loss_history, residual_loss_history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(loss_history)\n",
    "    plt.title('Total Loss vs Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Total Loss')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(boundary_loss_history)\n",
    "    plt.title('Boundary Loss vs Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Boundary Loss')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(residual_loss_history)\n",
    "    plt.title('Residual Loss vs Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Residual Loss')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('loss_vs_epochs.png')\n",
    "    plt.close()\n",
    "\n",
    "# Write loss data to CSV\n",
    "def write_loss_to_csv(loss_history, boundary_loss_history, residual_loss_history):\n",
    "    with open('loss_data.csv', 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Epoch', 'Total Loss', 'Boundary Loss', 'Residual Loss'])\n",
    "        for epoch, (total, boundary, residual) in enumerate(zip(loss_history, boundary_loss_history, residual_loss_history)):\n",
    "            csvwriter.writerow([epoch, total, boundary, residual])\n",
    "\n",
    "# Contour plot\n",
    "def plot_and_save_contour(model, x, t, usol):\n",
    "    # Create mesh grid\n",
    "    X, T = np.meshgrid(x.flatten(), t.flatten())\n",
    "    \n",
    "    # Prepare input for prediction\n",
    "    X_tensor = torch.tensor(X.reshape(-1, 1), dtype=torch.float32, device=device)\n",
    "    T_tensor = torch.tensor(T.reshape(-1, 1), dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        U_pred = model.net_u(X_tensor, T_tensor).cpu().numpy().reshape(X.shape)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Actual solution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.contourf(X, T, usol.T, levels=20, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.title('Actual Solution')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('t')\n",
    "    \n",
    "    # Predicted solution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.contourf(X, T, U_pred, levels=20, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.title('Predicted Solution')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('t')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('burgers_contour.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Save contours to .dat file\n",
    "    np.savetxt('burgers_contours.dat', np.column_stack((X.flatten(), T.flatten(), U_pred.flatten())), \n",
    "               header='x\\tt\\tU', comments='')\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize model on the device\n",
    "    model = FCN(2, 1, 20, 8, device=device).to(device)\n",
    "    \n",
    "    # Train PINN\n",
    "    loss_history, boundary_loss_history, residual_loss_history = train_pinn(model)\n",
    "    \n",
    "    # Plot and save loss data\n",
    "    plot_loss(loss_history, boundary_loss_history, residual_loss_history)\n",
    "    write_loss_to_csv(loss_history, boundary_loss_history, residual_loss_history)\n",
    "    \n",
    "    # Plot and save contour\n",
    "    plot_and_save_contour(model, x.cpu(), t.cpu(), usol.cpu())\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'burgers_pinn_model.pth')\n",
    "    \n",
    "    print(\"Training complete. Check the generated plots, CSV, and .dat files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way of computing the boundary conditions\n",
    "import torch\n",
    "\n",
    "# Define the neural network (NN)\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 50),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(50, 50),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(50, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Initialize NN\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "# Define the inputs for each term\n",
    "# Boundary condition inputs\n",
    "xj = torch.linspace(0, 1, 100).view(-1, 1).requires_grad_(True)  # x values for boundary condition\n",
    "tk = torch.zeros(50, 1).requires_grad_(True)  # t values for boundary at x=-1\n",
    "tl = torch.zeros(50, 1).requires_grad_(True)  # t values for boundary at x=+1\n",
    "\n",
    "# Interior points for physics-informed loss\n",
    "Np = 1000  # Number of interior points\n",
    "xi = torch.rand(Np, 1) * 2 - 1  # Random x values in [-1, 1]\n",
    "ti = torch.rand(Np, 1)  # Random t values in [0, 1]\n",
    "input_interior = torch.cat([xi, ti], dim=1)\n",
    "\n",
    "# Define lambda weights\n",
    "lambda_1, lambda_2, lambda_3 = 1.0, 1.0, 1.0  # Example weights\n",
    "\n",
    "# Compute boundary loss L_b\n",
    "NN_xj_0 = nn(torch.cat([xj, torch.zeros_like(xj)], dim=1))\n",
    "boundary1 = torch.mean((NN_xj_0 + torch.sin(torch.pi * xj))**2)\n",
    "\n",
    "NN_minus1_tk = nn(torch.cat([-torch.ones_like(tk), tk], dim=1))\n",
    "boundary2 = torch.mean((NN_minus1_tk - 0)**2)\n",
    "\n",
    "NN_plus1_tl = nn(torch.cat([torch.ones_like(tl), tl], dim=1))\n",
    "boundary3 = torch.mean((NN_plus1_tl - 0)**2)\n",
    "\n",
    "L_b = lambda_1 * boundary1 + lambda_2 * boundary2 + lambda_3 * boundary3\n",
    "\n",
    "# Compute physics-informed loss L_p\n",
    "NN_interior = nn(input_interior)\n",
    "\n",
    "# Partial derivatives\n",
    "dNN_dt = torch.autograd.grad(NN_interior, ti, grad_outputs=torch.ones_like(NN_interior), retain_graph=True, create_graph=True)[0]\n",
    "dNN_dx = torch.autograd.grad(NN_interior, xi, grad_outputs=torch.ones_like(NN_interior), retain_graph=True, create_graph=True)[0]\n",
    "d2NN_dx2 = torch.autograd.grad(dNN_dx, xi, grad_outputs=torch.ones_like(dNN_dx), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "# Physics loss\n",
    "nu = 0.01  # Viscosity\n",
    "physics_residual = dNN_dt + NN_interior * dNN_dx - nu * d2NN_dx2\n",
    "L_p = torch.mean(physics_residual**2)\n",
    "\n",
    "# Total loss\n",
    "total_loss = L_b + L_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the neural network architecture\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PINN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(2, 50),  # Input: (x, t)\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 1)   # Output: u(x, t)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Concatenate x and t along the last dimension\n",
    "        input_tensor = torch.cat([x, t], dim=1)\n",
    "        return self.layers(input_tensor)\n",
    "\n",
    "# Initialize the PINN\n",
    "model = PINN()\n",
    "\n",
    "# Define the boundary and physics-informed losses\n",
    "def loss_fn(model, xj, tk, tl, xi, ti, nu, lambda_1=1.0, lambda_2=1.0, lambda_3=1.0):\n",
    "    # Boundary Loss (Lb)\n",
    "    # u(x, 0) = -sin(pi*x)\n",
    "    NN_xj_0 = model(xj, torch.zeros_like(xj))\n",
    "    boundary1 = torch.mean((NN_xj_0 + torch.sin(torch.pi * xj))**2)\n",
    "    \n",
    "    # u(-1, t) = 0\n",
    "    NN_minus1_tk = model(-torch.ones_like(tk), tk)\n",
    "    boundary2 = torch.mean((NN_minus1_tk - 0)**2)\n",
    "    \n",
    "    # u(1, t) = 0\n",
    "    NN_plus1_tl = model(torch.ones_like(tl), tl)\n",
    "    boundary3 = torch.mean((NN_plus1_tl - 0)**2)\n",
    "\n",
    "    L_b = lambda_1 * boundary1 + lambda_2 * boundary2 + lambda_3 * boundary3\n",
    "\n",
    "    # Physics-Informed Loss (Lp)\n",
    "    NN_interior = model(xi, ti)\n",
    "    \n",
    "    # Compute derivatives using autograd\n",
    "    dNN_dt = torch.autograd.grad(NN_interior, ti, grad_outputs=torch.ones_like(NN_interior), retain_graph=True, create_graph=True)[0]\n",
    "    dNN_dx = torch.autograd.grad(NN_interior, xi, grad_outputs=torch.ones_like(NN_interior), retain_graph=True, create_graph=True)[0]\n",
    "    d2NN_dx2 = torch.autograd.grad(dNN_dx, xi, grad_outputs=torch.ones_like(dNN_dx), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "    # Physics residual\n",
    "    physics_residual = dNN_dt + NN_interior * dNN_dx - nu * d2NN_dx2\n",
    "    L_p = torch.mean(physics_residual**2)\n",
    "\n",
    "    # Total loss\n",
    "    return L_b + L_p\n",
    "\n",
    "# Training parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "nu = 0.01  # Viscosity\n",
    "epochs = 5000\n",
    "\n",
    "# Generate training data\n",
    "xj = torch.linspace(-1, 1, 100).view(-1, 1).requires_grad_(True)  # Boundary x points\n",
    "tk = torch.rand(50, 1).requires_grad_(True)  # Boundary t points\n",
    "tl = torch.rand(50, 1).requires_grad_(True)  # Boundary t points\n",
    "\n",
    "xi = torch.rand(1000, 1) * 2 - 1  # Interior x points in [-1, 1]\n",
    "ti = torch.rand(1000, 1)  # Interior t points in [0, 1]\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(model, xj, tk, tl, xi, ti, nu)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# After training, you can evaluate the model at desired points (x, t)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
